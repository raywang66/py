"""
Skin Color Matcher - Production-grade tool for portrait skin tone alignment
Author: CV/Image Processing Engineer
Hardware: NVIDIA GeForce RTX 3050 Ti (CUDA 12.8)

This tool analyzes skin color distributions between a reference JPEG/PNG and a Sony .ARW file,
providing Lightroom Classic HSL and Color Grading adjustment recommendations.
"""

import logging
import sys
from pathlib import Path
from typing import Tuple, Dict, Optional, Union
from dataclasses import dataclass

import numpy as np
import torch
import torch.nn.functional as F
from torch import Tensor
import rawpy
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from scipy import stats

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('skin_color_matcher.log')
    ]
)


@dataclass
class ColorStats:
    """Color statistics container"""
    h_mean: float
    s_mean: float
    l_mean: float
    h_std: float
    s_std: float
    l_std: float
    h_median: float
    s_median: float
    l_median: float
    shadow_h_mean: float  # Shadow region (L < 20%)
    shadow_s_mean: float
    total_pixels: int


@dataclass
class LightroomAdjustments:
    """Lightroom adjustment parameters"""
    # HSL Panel
    hsl_hue_orange: int  # -100 to +100
    hsl_hue_red: int
    hsl_sat_orange: int
    hsl_sat_red: int
    hsl_lum_orange: int
    hsl_lum_red: int

    # Color Grading
    shadows_hue: float  # 0-360
    shadows_sat: float  # 0-100
    midtones_hue: float
    midtones_sat: float
    highlights_hue: float
    highlights_sat: float


class SkinColorMatcher:
    """
    Production-grade skin color matcher with GPU acceleration.

    Analyzes skin tone differences between reference and test images,
    providing actionable Lightroom adjustment recommendations.
    """

    # Skin hue range in HSL (degrees)
    SKIN_HUE_MIN = 0
    SKIN_HUE_MAX = 50

    # Shadow threshold (L < 20%)
    SHADOW_THRESHOLD = 0.20

    # Face parsing class indices (BiSeNet standard)
    SKIN_CLASSES = [1, 2, 3, 10, 11, 12, 13]  # Face skin, left/right ear, neck
    EXCLUDE_CLASSES = [4, 5, 6, 7, 8, 9]  # Eyes, brows, nose, mouth, lips, hair

    def __init__(self, use_gpu: bool = True, model_path: Optional[str] = None):
        """
        Initialize the skin color matcher.

        Args:
            use_gpu: Enable CUDA acceleration if available
            model_path: Path to pretrained face parsing model (optional)
        """
        self.logger = logging.getLogger(self.__class__.__name__)

        # Device configuration
        self.device = self._setup_device(use_gpu)
        self.logger.info(f"Device configured: {self.device}")

        # Load face parsing model
        self.face_parser = self._load_face_parser(model_path)

        # Statistics cache
        self._ref_stats: Optional[ColorStats] = None
        self._test_stats: Optional[ColorStats] = None

    def _setup_device(self, use_gpu: bool) -> torch.device:
        """Configure computation device with CUDA support"""
        if use_gpu and torch.cuda.is_available():
            device = torch.device('cuda')
            # Log CUDA information
            self.logger.info(f"CUDA Device: {torch.cuda.get_device_name(0)}")
            self.logger.info(f"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            # Set memory allocation strategy for 4GB GPU
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            if use_gpu:
                self.logger.warning("CUDA not available, falling back to CPU")
        return device

    def _load_face_parser(self, model_path: Optional[str]) -> torch.nn.Module:
        """
        Load pretrained face parsing model (BiSeNet or similar).

        For production, use a pretrained BiSeNet model. This is a placeholder
        that creates a lightweight segmentation model.
        """
        self.logger.info("Loading face parsing model...")

        try:
            # In production, load actual BiSeNet model:
            # model = BiSeNet(n_classes=19)
            # model.load_state_dict(torch.load(model_path))

            # Placeholder: Simple U-Net-like architecture for demonstration
            # You should replace this with actual BiSeNet or face-parsing-PyTorch
            model = self._create_lightweight_parser()
            model.to(self.device)
            model.eval()

            self.logger.info("Face parsing model loaded successfully")
            return model

        except Exception as e:
            self.logger.error(f"Failed to load face parsing model: {e}")
            raise

    def _create_lightweight_parser(self) -> torch.nn.Module:
        """
        Lightweight face parser placeholder.

        PRODUCTION NOTE: Replace with actual BiSeNet from:
        https://github.com/zllrunning/face-parsing.PyTorch
        """
        class SimpleFaceParser(torch.nn.Module):
            def __init__(self):
                super().__init__()
                # Simplified encoder-decoder for face parsing
                self.encoder = torch.nn.Sequential(
                    torch.nn.Conv2d(3, 64, 3, padding=1),
                    torch.nn.ReLU(),
                    torch.nn.MaxPool2d(2),
                    torch.nn.Conv2d(64, 128, 3, padding=1),
                    torch.nn.ReLU(),
                    torch.nn.MaxPool2d(2),
                )
                self.decoder = torch.nn.Sequential(
                    torch.nn.Conv2d(128, 64, 3, padding=1),
                    torch.nn.ReLU(),
                    torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                    torch.nn.Conv2d(64, 19, 3, padding=1),  # 19 classes
                    torch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),
                )

            def forward(self, x):
                x = self.encoder(x)
                x = self.decoder(x)
                return x

        return SimpleFaceParser()

    def load_reference_image(self, image_path: Union[str, Path]) -> np.ndarray:
        """
        Load reference JPEG/PNG image.

        Args:
            image_path: Path to reference image

        Returns:
            RGB image array (H, W, 3) in range [0, 1]
        """
        try:
            image_path = Path(image_path)
            self.logger.info(f"Loading reference image: {image_path}")

            img = Image.open(image_path).convert('RGB')
            img_array = np.array(img, dtype=np.float32) / 255.0

            self.logger.info(f"Reference image loaded: {img_array.shape}")
            return img_array

        except Exception as e:
            self.logger.error(f"Failed to load reference image: {e}")
            raise

    def load_raw_image(self, raw_path: Union[str, Path], apply_gamma: bool = True) -> np.ndarray:
        """
        Load and process Sony .ARW RAW file with proper gamma correction.

        Critical: Apply gamma 2.2 to align brightness baseline with JPEG reference.

        Args:
            raw_path: Path to .ARW file
            apply_gamma: Apply gamma 2.2 correction (recommended)

        Returns:
            RGB image array (H, W, 3) in range [0, 1]
        """
        try:
            raw_path = Path(raw_path)
            self.logger.info(f"Loading RAW image: {raw_path}")

            with rawpy.imread(str(raw_path)) as raw:
                # Process RAW with specific parameters for color accuracy
                rgb = raw.postprocess(
                    gamma=(1, 1),  # Linear output first
                    no_auto_bright=True,  # Disable auto brightness
                    use_camera_wb=True,  # Use camera white balance
                    output_bps=16  # 16-bit output for precision
                )

            # Convert to float [0, 1]
            rgb = rgb.astype(np.float32) / 65535.0

            # Apply gamma 2.2 correction to match JPEG baseline
            if apply_gamma:
                self.logger.info("Applying Gamma 2.2 correction to RAW data")
                rgb = np.power(rgb, 1.0 / 2.2)

            self.logger.info(f"RAW image processed: {rgb.shape}")
            return rgb

        except Exception as e:
            self.logger.error(f"Failed to load RAW image: {e}")
            raise

    @torch.no_grad()
    def extract_skin_mask(self, image: np.ndarray) -> np.ndarray:
        """
        Extract precise skin mask using face parsing model.

        Excludes eyes, teeth, lips, hair, and background.
        GPU-accelerated inference with memory management.

        Args:
            image: RGB image array (H, W, 3) in range [0, 1]

        Returns:
            Binary skin mask (H, W) as boolean array
        """
        try:
            self.logger.info("Extracting skin mask with face parsing model...")

            # Prepare input tensor
            h, w = image.shape[:2]
            img_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)  # (1, 3, H, W)
            img_tensor = img_tensor.to(self.device)

            # Resize to model input size (typically 512x512)
            target_size = 512
            img_resized = F.interpolate(img_tensor, size=(target_size, target_size),
                                       mode='bilinear', align_corners=False)

            # Log CUDA memory usage
            if self.device.type == 'cuda':
                self.logger.info(f"CUDA Memory allocated: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB")

            # Inference
            parsing_result = self.face_parser(img_resized)
            parsing_map = torch.argmax(parsing_result, dim=1).squeeze(0)  # (512, 512)

            # Resize back to original size
            parsing_map = F.interpolate(
                parsing_map.unsqueeze(0).unsqueeze(0).float(),
                size=(h, w),
                mode='nearest'
            ).squeeze().long()

            # Create skin mask (only skin classes, exclude eyes, mouth, etc.)
            skin_mask = torch.zeros_like(parsing_map, dtype=torch.bool)
            for class_idx in self.SKIN_CLASSES:
                skin_mask |= (parsing_map == class_idx)

            # Move to CPU and convert to numpy
            skin_mask_np = skin_mask.cpu().numpy()

            # Post-processing: morphological operations to clean mask
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            skin_mask_np = cv2.morphologyEx(skin_mask_np.astype(np.uint8),
                                           cv2.MORPH_CLOSE, kernel).astype(bool)
            skin_mask_np = cv2.morphologyEx(skin_mask_np.astype(np.uint8),
                                           cv2.MORPH_OPEN, kernel).astype(bool)

            # Clear CUDA cache
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()

            skin_pixels = np.sum(skin_mask_np)
            self.logger.info(f"Skin mask extracted: {skin_pixels} pixels ({100*skin_pixels/(h*w):.2f}%)")

            return skin_mask_np

        except Exception as e:
            self.logger.error(f"Skin mask extraction failed: {e}")
            # Fallback: simple color-based skin detection
            self.logger.warning("Falling back to color-based skin detection")
            return self._fallback_skin_detection(image)

    def _fallback_skin_detection(self, image: np.ndarray) -> np.ndarray:
        """
        Fallback skin detection using color thresholds.
        Used when face parsing model fails.
        """
        # Convert to YCrCb color space (good for skin detection)
        img_uint8 = (image * 255).astype(np.uint8)
        ycrcb = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2YCrCb)

        # Skin color range in YCrCb
        lower = np.array([0, 133, 77], dtype=np.uint8)
        upper = np.array([255, 173, 127], dtype=np.uint8)

        mask = cv2.inRange(ycrcb, lower, upper)

        # Clean up mask
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

        return mask.astype(bool)

    def rgb_to_hsl(self, rgb: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Convert RGB to HSL color space.

        Optimized with NumPy broadcasting for high performance.

        Args:
            rgb: RGB array (..., 3) in range [0, 1]

        Returns:
            Tuple of (H, S, L) arrays where H is in [0, 360], S and L in [0, 1]
        """
        r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]

        max_c = np.maximum(np.maximum(r, g), b)
        min_c = np.minimum(np.minimum(r, g), b)
        delta = max_c - min_c

        # Lightness
        l = (max_c + min_c) / 2.0

        # Saturation
        s = np.zeros_like(l)
        mask = delta != 0
        s[mask] = delta[mask] / (1 - np.abs(2 * l[mask] - 1) + 1e-10)

        # Hue
        h = np.zeros_like(l)

        r_max = (max_c == r) & mask
        g_max = (max_c == g) & mask
        b_max = (max_c == b) & mask

        h[r_max] = 60 * (((g[r_max] - b[r_max]) / delta[r_max]) % 6)
        h[g_max] = 60 * (((b[g_max] - r[g_max]) / delta[g_max]) + 2)
        h[b_max] = 60 * (((r[b_max] - g[b_max]) / delta[b_max]) + 4)

        h[h < 0] += 360

        return h, s, l

    def compute_color_statistics(self, image: np.ndarray, mask: np.ndarray) -> ColorStats:
        """
        Compute comprehensive color statistics for masked skin region.

        Args:
            image: RGB image array (H, W, 3)
            mask: Binary mask (H, W)

        Returns:
            ColorStats object with HSL statistics
        """
        self.logger.info("Computing color statistics...")

        # Extract masked pixels
        skin_pixels = image[mask]  # (N, 3)

        if skin_pixels.shape[0] == 0:
            raise ValueError("No skin pixels detected in mask!")

        # Convert to HSL
        h, s, l = self.rgb_to_hsl(skin_pixels)

        # Overall statistics
        stats_obj = ColorStats(
            h_mean=float(np.mean(h)),
            s_mean=float(np.mean(s)),
            l_mean=float(np.mean(l)),
            h_std=float(np.std(h)),
            s_std=float(np.std(s)),
            l_std=float(np.std(l)),
            h_median=float(np.median(h)),
            s_median=float(np.median(s)),
            l_median=float(np.median(l)),
            shadow_h_mean=0.0,
            shadow_s_mean=0.0,
            total_pixels=skin_pixels.shape[0]
        )

        # Shadow region statistics (L < 20%)
        shadow_mask = l < self.SHADOW_THRESHOLD
        if np.sum(shadow_mask) > 0:
            stats_obj.shadow_h_mean = float(np.mean(h[shadow_mask]))
            stats_obj.shadow_s_mean = float(np.mean(s[shadow_mask]))
            self.logger.info(f"Shadow region: {np.sum(shadow_mask)} pixels")

        self.logger.info(f"Color statistics computed for {stats_obj.total_pixels} pixels")
        self.logger.info(f"HSL Mean: H={stats_obj.h_mean:.1f}Â°, S={stats_obj.s_mean:.3f}, L={stats_obj.l_mean:.3f}")

        return stats_obj

    def compute_lightroom_adjustments(self, ref_stats: ColorStats,
                                     test_stats: ColorStats) -> LightroomAdjustments:
        """
        Compute Lightroom HSL and Color Grading adjustments.

        Maps HSL statistical differences to actionable slider values.

        Args:
            ref_stats: Reference image statistics
            test_stats: Test image statistics

        Returns:
            LightroomAdjustments object with slider recommendations
        """
        self.logger.info("Computing Lightroom adjustment parameters...")

        # Compute deltas
        delta_h = ref_stats.h_mean - test_stats.h_mean
        delta_s = ref_stats.s_mean - test_stats.s_mean
        delta_l = ref_stats.l_mean - test_stats.l_mean

        # Shadow region deltas
        delta_shadow_h = ref_stats.shadow_h_mean - test_stats.shadow_h_mean
        delta_shadow_s = ref_stats.shadow_s_mean - test_stats.shadow_s_mean

        self.logger.info(f"Delta H: {delta_h:.2f}Â°, Delta S: {delta_s:.3f}, Delta L: {delta_l:.3f}")

        # HSL Panel Adjustments (skin tones typically in Orange/Red range)
        # Hue: Â±1Â° maps to approximately Â±5 slider units
        hsl_hue_orange = int(np.clip(delta_h * 5, -100, 100))
        hsl_hue_red = int(np.clip(delta_h * 4, -100, 100))

        # Saturation: Â±0.1 maps to approximately Â±50 slider units
        hsl_sat_orange = int(np.clip(delta_s * 500, -100, 100))
        hsl_sat_red = int(np.clip(delta_s * 450, -100, 100))

        # Luminance: Â±0.1 maps to approximately Â±50 slider units
        hsl_lum_orange = int(np.clip(delta_l * 500, -100, 100))
        hsl_lum_red = int(np.clip(delta_l * 450, -100, 100))

        # Color Grading (Color Wheel)
        # Shadows
        shadows_hue = (ref_stats.shadow_h_mean % 360) if ref_stats.shadow_h_mean > 0 else 0
        shadows_sat = np.clip(abs(delta_shadow_s) * 200, 0, 100)

        # Midtones (use overall mean)
        midtones_hue = (ref_stats.h_mean % 360)
        midtones_sat = np.clip(abs(delta_s) * 150, 0, 100)

        # Highlights (typically less saturated)
        highlights_hue = (ref_stats.h_mean % 360)
        highlights_sat = np.clip(abs(delta_s) * 100, 0, 100)

        adjustments = LightroomAdjustments(
            hsl_hue_orange=hsl_hue_orange,
            hsl_hue_red=hsl_hue_red,
            hsl_sat_orange=hsl_sat_orange,
            hsl_sat_red=hsl_sat_red,
            hsl_lum_orange=hsl_lum_orange,
            hsl_lum_red=hsl_lum_red,
            shadows_hue=shadows_hue,
            shadows_sat=shadows_sat,
            midtones_hue=midtones_hue,
            midtones_sat=midtones_sat,
            highlights_hue=highlights_hue,
            highlights_sat=highlights_sat
        )

        self.logger.info("Lightroom adjustments computed successfully")
        return adjustments

    def visualize_results(self, ref_img: np.ndarray, test_img: np.ndarray,
                         ref_mask: np.ndarray, test_mask: np.ndarray,
                         ref_stats: ColorStats, test_stats: ColorStats,
                         adjustments: LightroomAdjustments,
                         save_path: Optional[str] = None):
        """
        Create comprehensive visualization of analysis results.

        Args:
            ref_img: Reference image
            test_img: Test image
            ref_mask: Reference skin mask
            test_mask: Test skin mask
            ref_stats: Reference statistics
            test_stats: Test statistics
            adjustments: Lightroom adjustments
            save_path: Optional path to save figure
        """
        self.logger.info("Generating visualization...")

        fig = plt.figure(figsize=(20, 12))
        gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)

        # Row 1: Original images and masks
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.imshow(ref_img)
        ax1.set_title('Reference Image', fontsize=12, fontweight='bold')
        ax1.axis('off')

        ax2 = fig.add_subplot(gs[0, 1])
        ax2.imshow(test_img)
        ax2.set_title('Test Image (RAW)', fontsize=12, fontweight='bold')
        ax2.axis('off')

        ax3 = fig.add_subplot(gs[0, 2])
        ax3.imshow(ref_mask, cmap='gray')
        ax3.set_title('Reference Skin Mask', fontsize=12, fontweight='bold')
        ax3.axis('off')

        ax4 = fig.add_subplot(gs[0, 3])
        ax4.imshow(test_mask, cmap='gray')
        ax4.set_title('Test Skin Mask', fontsize=12, fontweight='bold')
        ax4.axis('off')

        # Row 2: HSL distributions
        ref_pixels = ref_img[ref_mask]
        test_pixels = test_img[test_mask]

        ref_h, ref_s, ref_l = self.rgb_to_hsl(ref_pixels)
        test_h, test_s, test_l = self.rgb_to_hsl(test_pixels)

        ax5 = fig.add_subplot(gs[1, 0])
        ax5.hist(ref_h, bins=50, alpha=0.6, label='Reference', color='blue', density=True)
        ax5.hist(test_h, bins=50, alpha=0.6, label='Test', color='red', density=True)
        ax5.set_xlabel('Hue (degrees)')
        ax5.set_ylabel('Probability Density')
        ax5.set_title('Hue Distribution', fontweight='bold')
        ax5.legend()
        ax5.grid(alpha=0.3)

        ax6 = fig.add_subplot(gs[1, 1])
        ax6.hist(ref_s, bins=50, alpha=0.6, label='Reference', color='blue', density=True)
        ax6.hist(test_s, bins=50, alpha=0.6, label='Test', color='red', density=True)
        ax6.set_xlabel('Saturation')
        ax6.set_ylabel('Probability Density')
        ax6.set_title('Saturation Distribution', fontweight='bold')
        ax6.legend()
        ax6.grid(alpha=0.3)

        ax7 = fig.add_subplot(gs[1, 2])
        ax7.hist(ref_l, bins=50, alpha=0.6, label='Reference', color='blue', density=True)
        ax7.hist(test_l, bins=50, alpha=0.6, label='Test', color='red', density=True)
        ax7.set_xlabel('Lightness')
        ax7.set_ylabel('Probability Density')
        ax7.set_title('Lightness Distribution', fontweight='bold')
        ax7.legend()
        ax7.grid(alpha=0.3)

        # Shadow region analysis
        ax8 = fig.add_subplot(gs[1, 3])
        ref_shadow = ref_l < self.SHADOW_THRESHOLD
        test_shadow = test_l < self.SHADOW_THRESHOLD
        if np.sum(ref_shadow) > 0 and np.sum(test_shadow) > 0:
            ax8.hist(ref_h[ref_shadow], bins=30, alpha=0.6, label='Ref Shadow', color='blue', density=True)
            ax8.hist(test_h[test_shadow], bins=30, alpha=0.6, label='Test Shadow', color='red', density=True)
        ax8.set_xlabel('Hue (degrees)')
        ax8.set_ylabel('Probability Density')
        ax8.set_title('Shadow Region Hue (L < 20%)', fontweight='bold')
        ax8.legend()
        ax8.grid(alpha=0.3)

        # Row 3: Lightroom adjustments display
        ax9 = fig.add_subplot(gs[2, :2])
        ax9.axis('off')

        hsl_text = f"""
LIGHTROOM HSL ADJUSTMENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
HSL Panel (Range: -100 to +100)

Orange Hue:        {adjustments.hsl_hue_orange:+4d}
Orange Saturation: {adjustments.hsl_sat_orange:+4d}
Orange Luminance:  {adjustments.hsl_lum_orange:+4d}

Red Hue:           {adjustments.hsl_hue_red:+4d}
Red Saturation:    {adjustments.hsl_sat_red:+4d}
Red Luminance:     {adjustments.hsl_lum_red:+4d}
        """

        ax9.text(0.1, 0.5, hsl_text, fontsize=11, fontfamily='monospace',
                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

        ax10 = fig.add_subplot(gs[2, 2:])
        ax10.axis('off')

        grading_text = f"""
COLOR GRADING (Color Wheels)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Shadows:
  Hue: {adjustments.shadows_hue:6.1f}Â° | Sat: {adjustments.shadows_sat:5.1f}

Midtones:
  Hue: {adjustments.midtones_hue:6.1f}Â° | Sat: {adjustments.midtones_sat:5.1f}

Highlights:
  Hue: {adjustments.highlights_hue:6.1f}Â° | Sat: {adjustments.highlights_sat:5.1f}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Statistics Comparison:
Ref H: {ref_stats.h_mean:6.1f}Â° | Test H: {test_stats.h_mean:6.1f}Â°
Ref S: {ref_stats.s_mean:6.3f}  | Test S: {test_stats.s_mean:6.3f}
Ref L: {ref_stats.l_mean:6.3f}  | Test L: {test_stats.l_mean:6.3f}
        """

        ax10.text(0.1, 0.5, grading_text, fontsize=11, fontfamily='monospace',
                 verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))

        plt.suptitle('Skin Color Analysis & Lightroom Adjustment Recommendations',
                    fontsize=16, fontweight='bold', y=0.98)

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            self.logger.info(f"Visualization saved to: {save_path}")

        plt.show()

    def analyze(self, reference_path: Union[str, Path], test_raw_path: Union[str, Path],
               output_dir: Optional[Union[str, Path]] = None) -> LightroomAdjustments:
        """
        Complete analysis pipeline.

        Args:
            reference_path: Path to reference JPEG/PNG
            test_raw_path: Path to test .ARW file
            output_dir: Optional directory for outputs

        Returns:
            LightroomAdjustments object
        """
        try:
            self.logger.info("=" * 80)
            self.logger.info("SKIN COLOR MATCHING ANALYSIS STARTED")
            self.logger.info("=" * 80)

            # Load images
            ref_img = self.load_reference_image(reference_path)
            test_img = self.load_raw_image(test_raw_path, apply_gamma=True)

            # Extract skin masks
            ref_mask = self.extract_skin_mask(ref_img)
            test_mask = self.extract_skin_mask(test_img)

            # Compute statistics
            ref_stats = self.compute_color_statistics(ref_img, ref_mask)
            test_stats = self.compute_color_statistics(test_img, test_mask)

            # Compute adjustments
            adjustments = self.compute_lightroom_adjustments(ref_stats, test_stats)

            # Visualize
            save_path = None
            if output_dir:
                output_dir = Path(output_dir)
                output_dir.mkdir(exist_ok=True)
                save_path = output_dir / 'skin_color_analysis.png'

            self.visualize_results(ref_img, test_img, ref_mask, test_mask,
                                  ref_stats, test_stats, adjustments, save_path)

            # Print final recommendations
            self._print_recommendations(adjustments)

            self.logger.info("=" * 80)
            self.logger.info("ANALYSIS COMPLETED SUCCESSFULLY")
            self.logger.info("=" * 80)

            return adjustments

        except Exception as e:
            self.logger.error(f"Analysis failed: {e}", exc_info=True)
            raise

    def _print_recommendations(self, adj: LightroomAdjustments):
        """Print formatted Lightroom recommendations"""
        print("\n" + "=" * 80)
        print("LIGHTROOM CLASSIC ADJUSTMENT RECOMMENDATIONS")
        print("=" * 80)
        print("\nðŸ“Š HSL PANEL:")
        print(f"  Orange â†’ Hue: {adj.hsl_hue_orange:+4d} | Saturation: {adj.hsl_sat_orange:+4d} | Luminance: {adj.hsl_lum_orange:+4d}")
        print(f"  Red    â†’ Hue: {adj.hsl_hue_red:+4d} | Saturation: {adj.hsl_sat_red:+4d} | Luminance: {adj.hsl_lum_red:+4d}")
        print("\nðŸŽ¨ COLOR GRADING:")
        print(f"  Shadows    â†’ Hue: {adj.shadows_hue:6.1f}Â° | Saturation: {adj.shadows_sat:5.1f}")
        print(f"  Midtones   â†’ Hue: {adj.midtones_hue:6.1f}Â° | Saturation: {adj.midtones_sat:5.1f}")
        print(f"  Highlights â†’ Hue: {adj.highlights_hue:6.1f}Â° | Saturation: {adj.highlights_sat:5.1f}")
        print("=" * 80 + "\n")


def main():
    """Example usage"""
    # Initialize matcher with GPU acceleration
    matcher = SkinColorMatcher(use_gpu=True)

    # Define paths (update with your actual paths)
    reference_image = "reference_portrait.jpg"  # JPEG/PNG reference
    test_raw_image = "test_portrait.ARW"         # Sony RAW file
    output_directory = "output"

    # Run analysis
    adjustments = matcher.analyze(
        reference_path=reference_image,
        test_raw_path=test_raw_image,
        output_dir=output_directory
    )

    return adjustments


if __name__ == "__main__":
    main()

